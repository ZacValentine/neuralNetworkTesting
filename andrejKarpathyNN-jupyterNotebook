import math
import numpy as np
import matplotlib.pyplot as plt
import math
import numpy as np
import matplotlib.pyplot as plt
def f(x):
    return 3*x**2 - 4*x + 5
f(3.0)
20.0
xs = np.arange(-5, 5, 0.25)
ys = f(xs)
plt.plot(xs, ys)
[<matplotlib.lines.Line2D at 0x401b758>]

h = 0.000001
x = 2/3
(f(x+h) - f(x)) / h
2.999378523327323e-06
a = 2.0
b = -3.0
c = 10.0
d = a*b + c
print(d)
4.0
h = 0.0001
​
# inputs
a = 2.0
b = -3.0
c = 10.0
​
d1 = a*b + c
# a+= h
#b += h
c += h
#derivatives with respect to each input - kind of a partial derivative - tells how the function changes with respect to each input
d2 = a*b +c
print('d1', d1)
print('d2', d2)
print('slope', (d2 - d1)/h)
​
​
​
#NOTHING MATTERS UNTIL THE NEXT NODE
d1 4.0
d2 4.0001
slope 0.9999999999976694
class Value:
​
    def __init__(self, data, _children=(), _op='', label=''):
        self.data = data
        self.grad = 0.0
        self._backward = lambda: None
        self._prev = set(_children)
        self._op = _op
        self.label = label
​
    def __repr__(self):
        return f"Value(data={self.data})"
​
    def __add__(self, other):
        out = Value(self.data + other.data, (self, other), '+')
        
        def _backward():
            self.grad = 1.0 * out.grad
            other.grad = 1.0 * out.grad
        out._backward = _backward
        return out
​
    def __mul__(self, other):
        out = Value(self.data * other.data, (self, other), '*')
        
        def _backward():
            self.grad = other.data * out.grad
            other.grad = self.data * out.grad
        out._backward = _backward
        
        return out
    
    def tanh(self):
        x = self.data
        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)
        out = Value(t, (self, ), 'tanh')
        
        def _backward():
            self.grad = (1 - t**2) * out.grad
        out._backward = _backward
        
        return out
​
​
a = Value(2.0, label='a')
b = Value(-3.0, label='b')
c = Value(10.0, label='c')
# a + b #same as - a.__add__(b)
e = a*b; e.label = 'e'
#d = a * b + c  # same as a.__mul(b).add(c)
d = e+c; d.label = 'd'
f = Value(-2.0, label='f')
L = d * f; L.label = 'L'
#take derivatives of each layer, with respect to the each thing on that layer
# dL/dd = d df/dd = f
# dL/df = d df/df = d
​
# dd/dc d = c + e D's rate of change with respect to C is 1 because it is just adding the e
# WANT dL/dc = (dL/dd) * (dd/dc) chain rule/related rates
# KNOW dL/dd and dd/dc
# (dL/dd) -2 * dd/dc = -2 * 1 = -2
​
# dL/da = (dL/de) * (de/da)
#  -2 * a*b = -2 * b = 6
#a.grad = -2.0 * -3.0
#b.grad = -2.0 * 2.0
​
a.grad = 6.0
b.grad = -4.0
​
c.grad = -2.0
e.grad = -2.0
​
f.grad = 4.0
d.grad = -2.0
​
L.grad = 1.0
a.data += 0.01 * a.grad
b.data += 0.01 * b.grad
c.data += 0.01 * c.grad
f.data += 0.01 * f.grad
​
e = a * b
d = e + c
L = d * f
​
print(L.data)
-47.183529994950106
def lol():
    h = 0.0001
    
    a = Value(2.0, label='a')
    b = Value(-3.0, label='b')
    c = Value(10.0, label='c')
    e = a*b; e.label = 'e'
    d = e+c; d.label = 'd'
    f = Value(-2.0, label='f')
    L = d * f; L.label = 'L'
    L1 = L.data
    
    a = Value(2.0, label='a')
    b = Value(-3.0, label='b')
    c = Value(10.0, label='c')
    e = a*b; e.label = 'e'
    d = e+c; d.label = 'd'
    f = Value(-2.0, label='f')
    L = d * f; L.label = 'L'
    L2 = L.data
    
    print((L2 - L1)/h)
lol()
0.0
# inputs x1, x2
x1 = Value(2.0, label='x1')
x2 = Value(0.0, label='x2')
# weights w1, w2
w1 = Value(-3.0, label='w1')
w2 = Value(1.0, label='w2')
# bias of the neuron
b = Value(6.8813735870195432, label='b')
# x1*w1 + x2*w2 + b
x1w1 = x1*w1; x1w1.label = 'x1*w1'
x2w2 = x2*w2; x2w2.label = 'x2*w2'
x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'
n = x1w1x2w2 + b; n.label = 'n'
o = n.tanh(); o.label = 'o'
o.grad = 1.0
o.grad = 1.0
o._backward()
n._backward()
b._backward() # nothing happends just for show
x1w1x2w1._backward()
x2w2._backward()
x1w1._backward()
​
x1.grad = w1.data * x1w1.grad
w1.grad = x2.data * x2w2.grad
# do/dx2 = w2 value * x2w2 deriv
x2.grad = w2.data * x2w2.grad
w2.gard = x2.data * x2w2.grad
x1w1.grad = 0.5
x2w2.grad = 0.5
x1w1x2w1.grad = 0.5
b.grad = 0.5
n.grad = 0.5
o.grad = 1.0
o.grad = 1.0
​
